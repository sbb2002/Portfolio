{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_ch6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZz/xGI4sb+hftaoerOQPQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbb2002/Portfolio/blob/main/.study/Tensorflow/ML_ch6_multinomial_n_softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxR4bIxrBAzP",
        "outputId": "1fda0ed3-d0dc-4891-9ce3-5a11ce76301e"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RMKBrksBcQk"
      },
      "source": [
        "# Multinomial classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbriVqRJCqJW"
      },
      "source": [
        "Binary classification graph가 각각 $A$, $B$, $C$가 있다고 하자.\n",
        "\n",
        "binary classification graph를 모두 수행하면 데이터 분포가 마치 지도 위의 국가영토 안에 있는 도시같이 생겼다. \\\n",
        "여튼 이 graph들을 각각 단독적으로 수행하자니 일이 n배가 된다. \\\n",
        "그러니 이 graph를 행렬로 묶어 일을 한번에 처리할 수 있다. \\\n",
        "행렬로 나타내면 다음과 같다.\n",
        "\n",
        "> $\\begin{pmatrix}\n",
        "w_{A1} & w_{A2} & w_{A3} \\\\\n",
        "w_{B1} & w_{B2} & w_{B3} \\\\\n",
        "w_{C1} & w_{C2} & w_{C3} \n",
        "\\end{pmatrix}$\n",
        "$\\begin{pmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2} \\\\\n",
        "x_{3}\n",
        "\\end{pmatrix}$\n",
        "$=\\begin{pmatrix}\n",
        "\\hat{y_{A}} \\\\\n",
        "\\hat{y_{B}} \\\\\n",
        "\\hat{y_{C}}\n",
        "\\end{pmatrix}$\n",
        "; A, B, C 는 label로, 3개의 binary classification graph\n",
        "\n",
        "전에는 $H(x)$를 sigmoid로 적용함으로써 각 graph의 예측값 $\\hat{y}$를 얻을 수 있었다. 그러나 기존의 예측값은 절대치여서 정수화된 데이터에서는 적합하지 않았고, 각 graph의 예측에서 실제값 $y$의 등장확률을 알 수는 없었다. \\\n",
        "남자=0, 여자=1인데 어떤 사람에 대한 예측값이 $\\hat{y}=0.3$ 이면 이 얼마나 황당한가;;\n",
        "\n",
        "그래서 이번에는 정수화된 데이터에 맞춰 제대로 예측해보자. \\\n",
        "예측값 $\\hat{y}$의 등장확률을 연산하기 위해 softmax를, 그리고 $\\hat{y}$를 점지(?)하기위해 one-hot encoding를 사용한다.\n",
        "\n",
        "---\n",
        "\n",
        "**가령 예를 들어,** \\\n",
        "sigmoid만 사용하여 hypothesis를 사용하여 예측한 결과\n",
        "> Score: [[20.0], [5.0], [1.0]]\n",
        "\n",
        "softmax와 one-hot encoding을 사용하여 예측한 결과\n",
        "> Probability: [[0.7], [0.2], [0.1]] \\\n",
        " 해석: 20점 등장확률=70%, 5점 등장확률=20%, 1점 등장확률=10%\n",
        "\n",
        "> One-Hot Encoding: [[1.0], [0.0], [0.0]] \\\n",
        "해석: 20점이 나올 확률이 가장 높다!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjml-z3xNJc1"
      },
      "source": [
        "## Cross-entropy cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj-csUE9NNAI"
      },
      "source": [
        "Multinomial classification에서의 cost함수는 아래와 같다.\n",
        "\n",
        "> $ \\begin{align*}\n",
        "D(S,L) &= - \\sum_{i} L_{i} \\circ log(S_{i}) & \\\\\n",
        "; & S(y) = 예측값(probability) & \\text{<- 기존 가설 } H(x) \\\\\n",
        "; & L = 실제값 & \\text{<- 기존 실제값 } y &\n",
        "\\end{align*} $\n",
        "\n",
        "\n",
        "\n",
        "왜 이런 cost를 쓰게 되었을까? 괄호를 다시 씌워보자. \\\n",
        "> $D(S,L) = \\sum_{i} L_{i} \\circ (-log(\\hat{y_{i}})) $ \\\n",
        "\n",
        "여기서 $\\circ\\$는 hadamard product로 \n",
        "element-wise product, point-wise product라고도 한다. \\\n",
        "내적과 달리 **같은 위치**의 elem끼리 곱하는 것이다.\n",
        "\n",
        "우선 이 식의 로그꼴을 보자. \\\n",
        "로그 안 값이 0 이면 $\\infty$, 1 이면 0이 되는 성질에 주목하면 된다. \\\n",
        "\n",
        "---\n",
        "\n",
        "**예를 들어,** 어떤 데이터에서 다음과 같은 값을 제공했다. \\\n",
        "> 실제값Y $ L = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $ \\\n",
        "옳은 예측값 $ \\hat{Y} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $ , \n",
        "틀린 예측값 $ \\hat{Y} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $ \n",
        "\n",
        "cost함수에 이 값들을 대입해보자. \\\n",
        "\n",
        "\n",
        "> $ \\begin{align*}\n",
        "D(S,L) &= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\circ (-log(\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix})) \\\\\n",
        "&= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\circ \\begin{pmatrix} \\infty \\\\ 0 \\end{pmatrix} \\\\\n",
        "&= \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = O\n",
        "\\end{align*} $\n",
        "\n",
        "옳은 예측값에 대해 cost에 대해 0, 즉 최소가 되어 선택된다.\n",
        "\n",
        "반대로 틀린 예측값을 넣으면 \\\n",
        "> $ \\begin{align*}\n",
        "D(S,L) &= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\circ (-log(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix})) \\\\\n",
        "&= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\circ \\begin{pmatrix} 0 \\\\ \\infty \\end{pmatrix} \\\\\n",
        "&= \\begin{pmatrix} 0 \\\\ \\infty \\end{pmatrix}\n",
        "\\end{align*} $\n",
        "\n",
        "틀린 예측값에 대한 cost는 발산(nan)하므로 적절히 걸러진다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCgDxVHHdHZ4"
      },
      "source": [
        "## Softmax function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t2_Bv-adW4i"
      },
      "source": [
        "아까 score(절대치)를 prob.(상대치)으로 바꿀 수 있다고 하지 않았는가? \\\n",
        "그 과정을 수행하는 함수가 바로 softmax function이다.\n",
        "\n",
        "1. **hypothesis**\n",
        "\n",
        " score는 기존의 $H(x) = WX + b$를 가설삼아 아래 코드를 이용한다.\n",
        "```\n",
        "tf.matmul(X,W) + b\n",
        "```\n",
        " prob.은 기존과 달리 $S(y_{i}) = \\frac{e^{y_{i}}}{\\sum e^{y_{i}}}$을 가설삼아 아래 코드를 이용한다.\n",
        "```\n",
        "tf.nn.softmax(tf.matmul(X,W) + b)\n",
        "```\n",
        "\n",
        "2. **cost(loss) function**\n",
        "\n",
        " $\\mathcal{L} = \\frac{1}{N} \\sum_{i} D(S(WX_{i}+b), L_{i})$ \n",
        "\n",
        " 수학적으로 이렇게 정의되며, 그 코드는 아래와 같다.\n",
        "```\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4n33B_zlA1t"
      },
      "source": [
        "## One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aqBePQ9lDHb"
      },
      "source": [
        " 숫자 0, 1, 2, 3를 원핫 인코딩으로 표현하면 각각 이렇게 표시된다.\n",
        " > [1,0,0,0]=0, [0,1,0,0]=1, [0,0,1,0]=2, [0,0,0,1]=3\n",
        "\n",
        " 마치 자리표같은 개념이다. \\\n",
        " [ ]안 0, 1은 occuipied 여부라고 생각하면 되겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_r7qO6qaJYW"
      },
      "source": [
        "## Practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWIJ3ySVfyDJ"
      },
      "source": [
        "# Give the data\n",
        "x_data = [[1,2,3,4], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7]]\n",
        "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
        "\n",
        "# Define variables\n",
        "X = tf.placeholder(\"float\", [None, 4])\n",
        "Y = tf.placeholder(\"float\", [None, 3])\n",
        "nb_classes = 3      # ont-hot label의 갯수\n",
        "\n",
        "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "\n",
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)\n",
        "\n",
        "# Cross entropy cost\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aePW9pW-n5NT",
        "outputId": "f0331aaa-2bc9-42ec-a5ae-61a3e8619d28"
      },
      "source": [
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for step in range(2001):\n",
        "    sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
        "  \n",
        "  print(\"*\"*100)\n",
        "\n",
        "  # Testing & one-hot encoding\n",
        "  print(\"\\ntest example1 )\")\n",
        "  a = sess.run(hypothesis, feed_dict={X: [[1,11,7,9]]})\n",
        "  print(\"Probabilities are \\t\", a, \"\\n나올 만한 숫자는 \\t\", sess.run(tf.arg_max(a, 1)))\n",
        "\n",
        "  print(\"\\ntest example2 )\")\n",
        "  all = sess.run(hypothesis, feed_dict={X: [[1,11,7,9],\n",
        "                                            [1,3,4,3],\n",
        "                                            [1,1,0,1]]})\n",
        "  print(\"Probabilities are \\n\", all, \"\\n나올 만한 숫자는 \\t\", sess.run(tf.arg_max(all, 1)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3.0920093\n",
            "200 0.76331615\n",
            "400 0.599558\n",
            "600 0.47218907\n",
            "800 0.36392462\n",
            "1000 0.27922708\n",
            "1200 0.24726559\n",
            "1400 0.22281599\n",
            "1600 0.20278364\n",
            "1800 0.18603793\n",
            "2000 0.17181705\n",
            "****************************************************************************************************\n",
            "\n",
            "test example1 )\n",
            "Probabilities are \t [[6.1080749e-03 9.9364674e-01 2.4517259e-04]] \n",
            "나올 만한 숫자는 \t [1]\n",
            "\n",
            "test example2 )\n",
            "Probabilities are \n",
            " [[6.1080749e-03 9.9364674e-01 2.4517259e-04]\n",
            " [7.9215646e-01 1.2212515e-01 8.5718378e-02]\n",
            " [6.7949757e-09 1.3661663e-04 9.9986339e-01]] \n",
            "나올 만한 숫자는 \t [1 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a6jE8gyuesg"
      },
      "source": [
        "# Fancy softmax classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6o1xgWPuo4m"
      },
      "source": [
        "## softmax_cross_entropy_with_logits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQUHEU8gpZOD"
      },
      "source": [
        "# logits = tf.matmul(X,W) + b         # scores(예측값 y) 와 동일\n",
        "# hypothesis = tf.nn.softmax(logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syKvTa1_zELa"
      },
      "source": [
        "# Cross entropy cost\n",
        "# cost = tf.reduce_mean(-tf.reduce_sum(Y_one_hot * tf.log(hypothesis), axis=1))\n",
        "\n",
        "# Cross entropy cost that is provided by tf.nn\n",
        "# cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)\n",
        "# cost = tf.reduce_mean(cost_i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm-JTgQ-9WhX"
      },
      "source": [
        "## tf.one_hot and reshape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wfbi3wt6PAA"
      },
      "source": [
        "### github에서 data 가져오는 방법\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls5nb57W1FRi",
        "outputId": "8bbdaa18-0b00-4d01-84c0-cd6590d228ba"
      },
      "source": [
        "# github에서 clone해와서 data 사용해보기\n",
        "!git clone https://github.com/hunkim/DeepLearningZeroToAll/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepLearningZeroToAll'...\n",
            "remote: Enumerating objects: 1709, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 1709 (delta 2), reused 1 (delta 1), pack-reused 1697\u001b[K\n",
            "Receiving objects: 100% (1709/1709), 751.23 KiB | 20.87 MiB/s, done.\n",
            "Resolving deltas: 100% (1111/1111), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP86JeIM3qsK",
        "outputId": "16374f04-1a59-4da7-dacc-afd1f566e26a"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DeepLearningZeroToAll  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_RecTvG5pIZ",
        "outputId": "73cf1d49-ad2a-43f0-95a3-2139d245397d"
      },
      "source": [
        "!ls DeepLearningZeroToAll"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " chainer\n",
            " CONTRIBUTING.md\n",
            " data-01-test-score.csv\n",
            " data-02-stock_daily.csv\n",
            " data-03-diabetes.csv\n",
            " data-04-zoo.csv\n",
            " ipynb\n",
            " keras\n",
            " lab-01-basics.ipynb\n",
            " lab-02-1-linear_regression.py\n",
            " lab-02-2-linear_regression_feed.py\n",
            " lab-02-3-linear_regression_tensorflow.org.py\n",
            " lab-03-1-minimizing_cost_show_graph.py\n",
            " lab-03-2-minimizing_cost_gradient_update.py\n",
            " lab-03-3-minimizing_cost_tf_optimizer.py\n",
            " lab-03-X-minimizing_cost_tf_gradient.py\n",
            " lab-04-1-multi_variable_linear_regression.py\n",
            " lab-04-2-multi_variable_matmul_linear_regression.py\n",
            " lab-04-3-file_input_linear_regression.py\n",
            " lab-04-4-tf_reader_linear_regression.py\n",
            " lab-05-1-logistic_regression.py\n",
            " lab-05-2-logistic_regression_diabetes.py\n",
            " lab-06-1-softmax_classifier.py\n",
            " lab-06-2-softmax_zoo_classifier.py\n",
            " lab-07-1-learning_rate_and_evaluation.py\n",
            " lab-07-2-linear_regression_without_min_max.py\n",
            " lab-07-3-linear_regression_min_max.py\n",
            " lab-07-4-mnist_introduction.py\n",
            " lab-08-tensor_manipulation.ipynb\n",
            " lab-09-1-xor.py\n",
            " lab-09-2-xor-nn.py\n",
            " lab-09-3-xor-nn-wide-deep.py\n",
            " lab-09-4-xor_tensorboard.py\n",
            " lab-09-5-linear_back_prop.py\n",
            " lab-09-6-multi-linear_back_prop.py\n",
            " lab-09-7-sigmoid_back_prop.py\n",
            " lab-09-x-xor-nn-back_prop.py\n",
            " lab-10-1-mnist_softmax.py\n",
            " lab-10-2-mnist_nn.py\n",
            " lab-10-3-mnist_nn_xavier.py\n",
            " lab-10-4-mnist_nn_deep.py\n",
            " lab-10-5-mnist_nn_dropout.py\n",
            " lab-10-6-mnist_nn_batchnorm.ipynb\n",
            " lab-10-7-mnist_nn_higher_level_API.py\n",
            "'lab-10-8-mnist_nn_selu(wip).py'\n",
            " lab-10-X1-mnist_back_prop.py\n",
            " lab-11-0-cnn_basics.ipynb\n",
            " lab-11-1-mnist_cnn.py\n",
            " lab-11-2-mnist_deep_cnn.py\n",
            " lab-11-3-mnist_cnn_class.py\n",
            " lab-11-4-mnist_cnn_layers.py\n",
            " lab-11-5-mnist_cnn_ensemble_layers.py\n",
            " lab-11-X-mnist_cnn_low_memory.py\n",
            " lab-12-0-rnn_basics.ipynb\n",
            " lab-12-1-hello-rnn.py\n",
            " lab-12-2-char-seq-rnn.py\n",
            " lab-12-3-char-seq-softmax-only.py\n",
            " lab-12-4-rnn_long_char.py\n",
            " lab-12-5-rnn_stock_prediction.py\n",
            " lab-13-1-mnist_using_scope.py\n",
            " lab-13-2-mnist_tensorboard.py\n",
            " lab-13-3-mnist_save_restore.py\n",
            " mxnet\n",
            " numpy\n",
            " pytorch\n",
            " README.md\n",
            " requirements.txt\n",
            " tests\n",
            " tf2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bGegKVO8QaX",
        "outputId": "2601bc9c-0266-4d06-95c1-d824b854a6e6"
      },
      "source": [
        "# 내가 찾는 파일경로 (출력되는 위치를 복사해 로드할 때 이용)\n",
        "!find ./ -name data-04-zoo.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./DeepLearningZeroToAll/mxnet/data-04-zoo.csv\n",
            "./DeepLearningZeroToAll/pytorch/data-04-zoo.csv\n",
            "./DeepLearningZeroToAll/keras/data-04-zoo.csv\n",
            "./DeepLearningZeroToAll/data-04-zoo.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szkW4FPp6lZC"
      },
      "source": [
        "위에서 가져온 데이터는 이렇게 이루어져있다고 한다. \\\n",
        "> [ : , 0:-1] = 동물들의 다리 수, 뿔이 있는지, 꼬리가 있는지 등을 feature화한 데이터, 16개 cols. \\\n",
        "[ : , [-1]] = 7개의 종으로 feature화한 데이터.\n",
        "\n",
        "다행히 여기서는 알려주었지만, \\\n",
        "원래는 pd나 np, excel 등을 이용하여 차원정보(row, col)와 목표로 할 데이터 범위(x_data, y_data)를 숙지해야한다.\n",
        "\n",
        "개인적으로 pd가 그래픽도 좋고 명확해서 좋긴 하지만, 연습 차 np로 분석해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGs2dGaINltF"
      },
      "source": [
        "### Practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve1366wz58mY"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSeve15_6LlO",
        "outputId": "50a54402-b63e-4f5c-e553-f3201613029c"
      },
      "source": [
        "# Predicting animal type based on various features\n",
        "xy = np.loadtxt('./DeepLearningZeroToAll/data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
        "np.array(xy).shape        # (rows= 101, cols= 17)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7L4CBnE8tWc"
      },
      "source": [
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "nb_classes = 7      # feature 0~6\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 16])\n",
        "Y = tf.placeholder(tf.int32, [None, 1])     # 0~6, shape=[?,1] ; None or ? = 데이터 n(rows)개가 들어온다\n",
        "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "\n",
        "Y_one_hot = tf.one_hot(Y, nb_classes)       # one hot shape=[?, 1, 7]... We don't want this.\n",
        "# After being one_hot, N-rank of input indices gives (N+1)-rank of output indices. \n",
        "# It means [[0], [3]] outputs one_hot[[[1,0,0,0,0,0,0]], [0,0,0,1,0,0,0]]]\n",
        "# So reshape Y_one_hot.\n",
        "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])   # shape=[?, 7] ; -1 은 나머지 모두\n",
        "\n",
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits)/ reduce_sum(exp(logits), dim)\n",
        "logits = tf.matmul(X,W) + b\n",
        "hypothesis = tf.nn.softmax(logits)\n",
        "\n",
        "# Cross entropy cost\n",
        "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)\n",
        "cost = tf.reduce_mean(cost_i)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "prediction = tf.argmax(hypothesis, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgK_YDsz-Pm-",
        "outputId": "fc11aab3-9d9b-4ee4-d26b-b9aaca1bed84"
      },
      "source": [
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  feed_dict = {X: x_data, Y: y_data}\n",
        "  for step in range(2000):\n",
        "    sess.run(optimizer, feed_dict=feed_dict)\n",
        "    if step % 100 == 0:\n",
        "      loss, acc = sess.run([cost, accuracy], feed_dict=feed_dict)\n",
        "      print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))    # 포맷 문자열; 5칸여백, 소수점 3자리고정, 소수점 2자리고정 퍼센트 (기본: 소수점 내림)\n",
        "  \n",
        "  # Let's see if we can predict\n",
        "  pred = sess.run(prediction, feed_dict={X: x_data})\n",
        "  # y_data: [N, 1] = flatten => [N, ] matches pred.shape\n",
        "  # example) [[1], [0]] flatten => [1, 0]\n",
        "  for p, y in zip(pred, y_data.flatten()):       # zip은 각각 list의 elems들을 p, y로 넘겨주기 위함\n",
        "    print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:     0\tLoss: 5.218\tAcc: 10.89%\n",
            "Step:   100\tLoss: 0.576\tAcc: 85.15%\n",
            "Step:   200\tLoss: 0.389\tAcc: 87.13%\n",
            "Step:   300\tLoss: 0.298\tAcc: 92.08%\n",
            "Step:   400\tLoss: 0.244\tAcc: 92.08%\n",
            "Step:   500\tLoss: 0.207\tAcc: 95.05%\n",
            "Step:   600\tLoss: 0.179\tAcc: 96.04%\n",
            "Step:   700\tLoss: 0.158\tAcc: 96.04%\n",
            "Step:   800\tLoss: 0.142\tAcc: 97.03%\n",
            "Step:   900\tLoss: 0.128\tAcc: 98.02%\n",
            "Step:  1000\tLoss: 0.117\tAcc: 98.02%\n",
            "Step:  1100\tLoss: 0.107\tAcc: 99.01%\n",
            "Step:  1200\tLoss: 0.099\tAcc: 99.01%\n",
            "Step:  1300\tLoss: 0.092\tAcc: 100.00%\n",
            "Step:  1400\tLoss: 0.085\tAcc: 100.00%\n",
            "Step:  1500\tLoss: 0.080\tAcc: 100.00%\n",
            "Step:  1600\tLoss: 0.075\tAcc: 100.00%\n",
            "Step:  1700\tLoss: 0.071\tAcc: 100.00%\n",
            "Step:  1800\tLoss: 0.067\tAcc: 100.00%\n",
            "Step:  1900\tLoss: 0.064\tAcc: 100.00%\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wZd9-w-cwZU"
      },
      "source": [
        "Step 1300 언저리에서 acc 100%를 달성한다. 충분히 훈련이 되어 train set에 대해서는 천하무적이 된 것이다. \\\n",
        "유머러스하게도, test set으로 자기자신(train set)을 넣었으니 모든 데이터에 대해 완벽한 예측이 가능하다. \\\n",
        "그래서 [False] 하나 없이 [True]만 나오는 것이다. \\\n",
        "\n",
        "당연하게도 실제 training에서는 절대 train set과 test set을 **똑같이 주어선 안된다!**\n",
        "\n",
        "나는 책에서 공부한 대로, data의 80%는 train set, 20%를 test set으로 떼어놓는다. \\\n",
        "ML_prac1에 그 코드를 활용했으니 참고하자."
      ]
    }
  ]
}